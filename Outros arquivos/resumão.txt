Slide 1:
Uma breve apresentação do algoritmo:
	O SA tenta fugir de máximos e mínimos locais por meio da aceitação de vizinhos piores que a solução atual. Esses vizinhos piores são aceitos com uma dada probabilidade que vai diminuindo ao longo da execução do algoritmo, conforme a temperatura do algoritmo vai diminuindo.

Slide 2:
Pra aplicar o método ao problema dos grupos balanceados de maior valor eu tive que definir algumas coisas:
	A representação de uma solução é uma lista de listas, em que cada lista representa um grupo e os elementos dentro de uma lista são os vértices que estão nesse grupo
	A solução inicial é gerada de forma aleatória.
	A vizinhança que eu escolhi utilizar foi o k-change. A cada vez que o algoritmo cria um vizinho, eu mudo k vértices aleatórios de lugar. No caso, o vértice é escolhido aleatoriamente e o novo grupo pra qual ele vai também é escolhido aleatoriamente.
	Um ponto bem importante do simulated annealing é definir a temperatura inicial do algoritmo e a final, que define quando ele deve parar. Se a temperatura final for muito baixa, o algoritmo passa a se comportar como uma busca local, e se a temperatura inicial for muito alta o algoritmo perde tempo fazendo uma busca aleatória no espaço de possíveis soluções do problema. Pra definir esses dois parâmetros, a gente pode usar o próprio algoritmo. Para a TI, a gente define uma probabilidade inicial que queremos que o algoritmo aceite um vizinho ruim, e executamos o algoritmo até chegar em uma temperatura na qual um vizinho pior foi aceito com probabilidade próxima o suficiente da probabilidade definida; a gente retorna essa temperatura pra execução real do SA. Para a TF, a gente simplesmente continua executando o algoritmo até chegarmos numa temperatura em que a aceitação de um vizinho pior ocorreu com probabilidade menor que a probabilidade final definida pelo usuário. Vale ressaltar também que falar em probabilidade de se aceitar um vizinho pior é muito menos abstrato do que falar em temperatura do algoritmo.

Slide 3:
Aqui começam os testes dos parâmetros. O primeiro testado foi o fator de resfriamento; conforme ele aumenta, o valor da solução final encontrada aumenta de forma quase linear. O aumento no tempo de execução, contudo, é bastante considerável.

Slide 4:
Bom, um negócio que eu acho que esqueci de falar é que, pra cada temperatura pela qual o algoritmo passa, a geração de vizinhos é realizada I vezes. Por esse gráfico aqui fica claro que esse parâmetro é um fator bem importante pro algoritmo; até 1000 há um salto bem grande na qualidade da solução final, que depois passa a crescer bem devagar. O tempo, por sua vez, continua aumentando de forma linear, então não vale a pena pegar um I muito grande. O que eu acabei pegando foi o próprio 2500.

Slide 5:
Pra probabilidade inicial os valores da solução final variaram bastante, enquanto o tempo aumenta conforme a probabilidade inicial aumenta. Eu optei por escolher como pi 0.95, já que foi oq obteve os melhores resultados e a penalidade no tempo não era tão grande assim.

Slide 6:
Probabilidade final:
Os gráficos deixaram bem óbvios que eu deveria escolher 0.01, pq a solução é a maior e o tempo ainda é bem aceitável.

Slide 7:
Bom, esse parâmetro aqui não diz muito respeito ao simulated annealing, mas eu resolvi testar também pra ver no que daria. Basicamente eu criei um dado número de solucoes iniciais, e a melhor delas eu passava pro simulated annealing. Como é esperado, isso não influi muito no simulated annealing. Criando apenas uma solucao inicial o algoritmo chegou em resultados melhores do que quando criei 1000. Além disso, o número de solucoes iniciais não afetou em praticamente nada o tempo de execucao do algoritmo, pq apenas 1 sol inicial foi mais lento do que 10000.

Slide 8:
Além disso, como também é esperado, o valor das soluções iniciais nunca difere muito.

Slide 9:
Eu testei diferentes valores de k para o k-change também. Aqui eu tive que tomar cuidado pra não escolher um k muito alto, porque isso faria com que a criação de vizinhos fosse basicamente um passeio aleatório sobre as possíveis soluções do problema, e o conceito de vizinhança meio que seria perdido. Então eu testei com k's bem baixos; deles, o melhor foi k=2, então permaneci com ele.

Slide 10:
Aqui começam os resultados. Em primeiro lugar, os resultados do solver não vao aparecer aqui, já que ele simplesmente não encontrou solucao factivel pra nenhuma das instâncias. Bom, com essa primeira tabela fica claro que o algoritmo melhorava bastante em relação à solúção inicial; fica claro, também, que ele demorou bastante, chegando a 3 horas e pouco.

Slide 11:
Aqui os meus resultados em comparação ao BKV. Dá pra ver que o algoritmo chegou bem próximo para as instancias com 240 vértices, e ficou um pouco mais afastado pra 480.

Slide 12:
Bom, com base nesses resultados, considero que o algoritmo obteve resultados super bons para as instâncias com 240 vértices e, embora um pouco piores para 480 vértices, ainda assim foram bem satisfatórios. Além disso, os resultados foram muito melhores do que os obtidos pelo solver, já que o solver nem conseguia achar uma solução factível. Com isso, acho que o trabalho foi bem-sucedido.